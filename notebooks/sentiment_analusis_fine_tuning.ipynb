{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Fine-Tuning\n",
    "## Complete Guide with Hands-on Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers datasets pandas numpy scikit-learn matplotlib seaborn tqdm accelerate evaluate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_model import download_and_setup_model\n",
    "\n",
    "# Download the main model\n",
    "model_path = download_and_setup_model()\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "df = pd.read_csv('../data/raw_data.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "label_counts.plot(kind='bar', color=['red', 'gray', 'green'])\n",
    "plt.title('Label Distribution')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(label_counts.values, labels=['Negative', 'Neutral', 'Positive'], \n",
    "        autopct='%1.1f%%', colors=['red', 'gray', 'green'])\n",
    "plt.title('Label Proportions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df['text_length'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df.boxplot(column='text_length', by='label')\n",
    "plt.title('Text Length by Sentiment')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sentiment_lengths = df.groupby('label')['text_length'].mean()\n",
    "sentiment_lengths.plot(kind='bar', color=['red', 'gray', 'green'])\n",
    "plt.title('Average Text Length by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average Length')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Max text length: {df['text_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import DataPreprocessor\n",
    "from config import config\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Check the current dataset first\n",
    "df = pd.read_csv('../data/raw_data.csv')\n",
    "print(f\"Current dataset size: {len(df)}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# If dataset is too small, let's create an expanded version\n",
    "if len(df) < 30:\n",
    "    print(\"\\nDataset is small. Creating expanded dataset...\")\n",
    "    \n",
    "    # Expanded dataset with more samples\n",
    "    expanded_data = [\n",
    "        # Negative samples (label 0)\n",
    "        {\"text\": \"This is terrible and I hate it.\", \"label\": 0},\n",
    "        {\"text\": \"Very poor quality and bad service.\", \"label\": 0},\n",
    "        {\"text\": \"Disappointing experience overall.\", \"label\": 0},\n",
    "        {\"text\": \"Worst purchase I've ever made.\", \"label\": 0},\n",
    "        {\"text\": \"Not worth the money at all.\", \"label\": 0},\n",
    "        {\"text\": \"Absolutely awful product.\", \"label\": 0},\n",
    "        {\"text\": \"I regret buying this.\", \"label\": 0},\n",
    "        {\"text\": \"Poor craftsmanship and design.\", \"label\": 0},\n",
    "        {\"text\": \"Complete waste of time and money.\", \"label\": 0},\n",
    "        {\"text\": \"Extremely dissatisfied with this product.\", \"label\": 0},\n",
    "        \n",
    "        # Neutral samples (label 1)\n",
    "        {\"text\": \"It's okay, nothing special.\", \"label\": 1},\n",
    "        {\"text\": \"Average product for the price.\", \"label\": 1},\n",
    "        {\"text\": \"Meets basic expectations.\", \"label\": 1},\n",
    "        {\"text\": \"Neither good nor bad.\", \"label\": 1},\n",
    "        {\"text\": \"Standard quality, as expected.\", \"label\": 1},\n",
    "        {\"text\": \"Adequate but not impressive.\", \"label\": 1},\n",
    "        {\"text\": \"Fair product overall.\", \"label\": 1},\n",
    "        {\"text\": \"Does the job, but could be better.\", \"label\": 1},\n",
    "        {\"text\": \"Moderate performance for daily use.\", \"label\": 1},\n",
    "        {\"text\": \"Satisfactory but not exceptional.\", \"label\": 1},\n",
    "        \n",
    "        # Positive samples (label 2)\n",
    "        {\"text\": \"Excellent product and great value!\", \"label\": 2},\n",
    "        {\"text\": \"I love this, it's amazing!\", \"label\": 2},\n",
    "        {\"text\": \"Outstanding quality and service.\", \"label\": 2},\n",
    "        {\"text\": \"Highly recommended to everyone.\", \"label\": 2},\n",
    "        {\"text\": \"Exceeded all my expectations.\", \"label\": 2},\n",
    "        {\"text\": \"Fantastic experience from start to finish.\", \"label\": 2},\n",
    "        {\"text\": \"Brilliant product with great features.\", \"label\": 2},\n",
    "        {\"text\": \"Perfect in every way!\", \"label\": 2},\n",
    "        {\"text\": \"Superb quality and excellent performance.\", \"label\": 2},\n",
    "        {\"text\": \"Wonderful product that delivers perfectly.\", \"label\": 2},\n",
    "    ]\n",
    "    \n",
    "    # Create expanded dataframe and combine with existing data\n",
    "    expanded_df = pd.DataFrame(expanded_data)\n",
    "    final_df = pd.concat([df, expanded_df], ignore_index=True)\n",
    "    \n",
    "    # Remove any duplicates\n",
    "    final_df = final_df.drop_duplicates(subset=['text'])\n",
    "    \n",
    "    print(f\"Expanded dataset size: {len(final_df)}\")\n",
    "    print(\"Expanded label distribution:\")\n",
    "    print(final_df['label'].value_counts().sort_index())\n",
    "    \n",
    "    # Save the expanded dataset\n",
    "    final_df.to_csv('../data/expanded_data.csv', index=False)\n",
    "    print(\"Expanded dataset saved to '../data/expanded_data.csv'\")\n",
    "    \n",
    "    # Use the expanded dataset\n",
    "    data_file = '../data/expanded_data.csv'\n",
    "else:\n",
    "    data_file = '../data/raw_data.csv'\n",
    "\n",
    "# Prepare datasets\n",
    "print(f\"\\nUsing dataset: {data_file}\")\n",
    "datasets = preprocessor.prepare_datasets(data_file)\n",
    "\n",
    "print(\"\\nDataset preparation completed!\")\n",
    "print(f\"Train set: {len(datasets['train'])} samples\")\n",
    "print(f\"Validation set: {len(datasets['validation'])} samples\")\n",
    "print(f\"Test set: {len(datasets['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore tokenized data\n",
    "print(\"Tokenized sample:\")\n",
    "sample = datasets['train'][0]\n",
    "for key in sample:\n",
    "    if key != 'text':\n",
    "        print(f\"{key}: {sample[key]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {sample[key][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training import SentimentTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SentimentTrainer(datasets)\n",
    "\n",
    "# Check model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(trainer.model)\n",
    "\n",
    "print(f\"\\nDevice: {trainer.device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training in notebook\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setting up model and training...\")\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(datasets['train'], batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(datasets['validation'], batch_size=16)\n",
    "test_dataloader = DataLoader(datasets['test'], batch_size=16)\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")\n",
    "\n",
    "# Setup training\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Prepare inputs correctly - THIS IS THE KEY FIX\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass - CORRECT WAY\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    val_loss = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        val_loss += outputs.loss.item()\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=all_predictions, references=all_labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=all_predictions, references=all_labels, average=\"weighted\")[\"f1\"]\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Val Accuracy: {accuracy:.4f}, Val F1: {f1:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Save model\n",
    "model_save_path = \"../models/saved_models/fine_tuned_sentiment\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"✅ Model saved successfully to {model_save_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import ModelEvaluator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator('../models/saved_models/fine_tuned_sentiment', datasets)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "try:\n",
    "    predictions, labels, probabilities = evaluator.evaluate_model()\n",
    "    print(\"✅ Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during evaluation: {e}\")\n",
    "    print(\"Trying alternative evaluation approach...\")\n",
    "    \n",
    "    # Alternative evaluation approach\n",
    "    test_dataloader = DataLoader(\n",
    "        datasets['test'],\n",
    "        batch_size=16,\n",
    "    )\n",
    "    \n",
    "    evaluator.model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        # Separate inputs and labels correctly\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(evaluator.device),\n",
    "            'attention_mask': batch['attention_mask'].to(evaluator.device)\n",
    "        }\n",
    "        labels_batch = batch['label'].to(evaluator.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = evaluator.model(**inputs)  # No labels during inference\n",
    "        \n",
    "        probabilities_batch = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predictions_batch = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions_batch.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities_batch.cpu().numpy())\n",
    "    \n",
    "    # Now generate reports manually\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(all_labels, all_predictions, \n",
    "                              target_names=['Negative', 'Neutral', 'Positive']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "               yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    predictions = all_predictions\n",
    "    labels = all_labels\n",
    "    probabilities = all_probabilities\n",
    "    \n",
    "    print(\"✅ Alternative evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import SentimentPredictor\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Initialize predictor with your fine-tuned model\n",
    "try:\n",
    "    predictor = SentimentPredictor('../models/saved_models/fine_tuned_sentiment')\n",
    "    print(\"✅ Predictor initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing predictor: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # Alternative initialization\n",
    "    model_path = '../models/saved_models/fine_tuned_sentiment'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"✅ Model loaded successfully for inference!\")\n",
    "\n",
    "# Test predictions on various examples\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! It's fantastic and works perfectly!\",\n",
    "    \"This is okay, nothing special but gets the job done.\",\n",
    "    \"Terrible quality, very disappointed with this purchase.\",\n",
    "    \"The service was excellent and the staff was very friendly and helpful.\",\n",
    "    \"It's mediocre at best, I expected more for the price.\",\n",
    "    \"Outstanding performance and incredible value for money!\",\n",
    "    \"Not good, not bad, just average in every way.\",\n",
    "    \"Absolutely horrible experience from start to finish.\",\n",
    "    \"Pretty good overall with some minor issues that could be improved.\",\n",
    "    \"This is the best thing I've ever bought! Highly recommended!\"\n",
    "]\n",
    "\n",
    "print(\"SENTIMENT ANALYSIS PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    try:\n",
    "        result = predictor.predict_sentiment(text)\n",
    "        print(f\"\\n{i}. {text}\")\n",
    "        print(f\"   → Prediction: {result['prediction']}\")\n",
    "        print(f\"   → Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"   → Probabilities: Negative: {result['probabilities']['Negative']:.4f}, \"\n",
    "              f\"Neutral: {result['probabilities']['Neutral']:.4f}, \"\n",
    "              f\"Positive: {result['probabilities']['Positive']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error predicting text {i}: {e}\")\n",
    "        # Manual prediction as fallback\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        \n",
    "        label_names = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "        \n",
    "        print(f\"\\n{i}. {text}\")\n",
    "        print(f\"   → Prediction: {label_names[prediction]}\")\n",
    "        print(f\"   → Confidence: {confidence:.4f}\")\n",
    "        print(f\"   → Probabilities: Negative: {probabilities[0][0].item():.4f}, \"\n",
    "              f\"Neutral: {probabilities[0][1].item():.4f}, \"\n",
    "              f\"Positive: {probabilities[0][2].item():.4f}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualizations\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Probability distributions\n",
    "prob_array = np.array(probabilities)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, sentiment in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(prob_array[:, i], bins=20, alpha=0.7, color=['red', 'gray', 'green'][i])\n",
    "    plt.title(f'{sentiment} Probability Distribution')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import SentimentPredictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = SentimentPredictor('../models/saved_models/fine_tuned_sentiment')\n",
    "\n",
    "# Test predictions\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! It's fantastic!\",\n",
    "    \"This is okay, nothing special.\",\n",
    "    \"Terrible quality, very disappointed.\",\n",
    "    \"The service was excellent and the staff was friendly.\",\n",
    "    \"It's mediocre at best, I expected more.\"\n",
    "]\n",
    "\n",
    "print(\"Real-time Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predictor.predict_sentiment(text)\n",
    "    predictor.print_prediction(result)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive sentiment analysis\n",
    "print(\"INTERACTIVE SENTIMENT ANALYSIS\")\n",
    "print(\"Type your text and press Enter to analyze sentiment\")\n",
    "print(\"Type 'quit' or 'exit' to stop\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"Enter text: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        result = predictor.predict_sentiment(user_input)\n",
    "        \n",
    "        print(f\"\\n📊 Analysis Results:\")\n",
    "        print(f\"   Text: {user_input}\")\n",
    "        print(f\"   Sentiment: {result['prediction']}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"   Detailed Probabilities:\")\n",
    "        for sentiment, prob in result['probabilities'].items():\n",
    "            print(f\"     {sentiment}: {prob:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"Please try again with different text.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Summary\n",
    "print(\"FINE-TUNED SENTIMENT ANALYSIS MODEL - PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate final metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Use the predictions from our evaluation\n",
    "if 'predictions' in locals() and 'labels' in locals():\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    \n",
    "    print(f\"\\n📈 TEST SET PERFORMANCE:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Weighted F1-Score: {f1:.4f}\")\n",
    "    print(f\"   Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(f\"\\n🎯 PER-CLASS PERFORMANCE:\")\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    for i, sentiment in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "        print(f\"   {sentiment}:\")\n",
    "        print(f\"     Precision: {precision_per_class[i]:.4f}\")\n",
    "        print(f\"     Recall: {recall_per_class[i]:.4f}\")\n",
    "        print(f\"     F1-Score: {f1_per_class[i]:.4f}\")\n",
    "        print(f\"     Support: {support_per_class[i]} samples\")\n",
    "\n",
    "# Model information\n",
    "print(f\"\\n🤖 MODEL INFORMATION:\")\n",
    "print(f\"   Base Model: distilbert-base-uncased\")\n",
    "print(f\"   Fine-tuned on: {len(datasets['train'])} training samples\")\n",
    "print(f\"   Validation samples: {len(datasets['validation'])}\")\n",
    "print(f\"   Test samples: {len(datasets['test'])}\")\n",
    "print(f\"   Number of classes: 3 (Negative, Neutral, Positive)\")\n",
    "print(f\"   Model saved at: ../models/saved_models/fine_tuned_sentiment\")\n",
    "\n",
    "print(f\"\\n✅ PROJECT STATUS: COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   You can now use your fine-tuned sentiment analysis model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with different models (optional)\n",
    "def compare_models():\n",
    "    model_results = {}\n",
    "    \n",
    "    # Test different models\n",
    "    models_to_test = {\n",
    "        'distilbert': 'distilbert-base-uncased',\n",
    "        'bert': 'bert-base-uncased',\n",
    "    }\n",
    "    \n",
    "    for model_name, model_path in models_to_test.items():\n",
    "        try:\n",
    "            print(f\"\\nTesting {model_name}...\")\n",
    "            \n",
    "            # Load model\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            \n",
    "            # Move to device\n",
    "            model.to(trainer.device)\n",
    "            \n",
    "            # Test on a few samples\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i in range(min(10, len(datasets['test']))):\n",
    "                sample = datasets['test'][i]\n",
    "                text = sample['text']\n",
    "                true_label = sample['label']\n",
    "                \n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                inputs = {k: v.to(trainer.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                pred_label = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                \n",
    "                if pred_label == true_label:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "            accuracy = correct / total\n",
    "            model_results[model_name] = accuracy\n",
    "            print(f\"{model_name} accuracy: {accuracy:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {e}\")\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "# Uncomment to run model comparison\n",
    "# results = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for production use - CORRECTED VERSION\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def export_for_production():\n",
    "    \"\"\"Export the model and create a simple deployment package\"\"\"\n",
    "    \n",
    "    deployment_dir = \"../models/deployment_package\"\n",
    "    os.makedirs(deployment_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy model files\n",
    "    model_source = \"../models/saved_models/fine_tuned_sentiment\"\n",
    "    model_dest = f\"{deployment_dir}/sentiment_model\"\n",
    "    \n",
    "    if os.path.exists(model_source):\n",
    "        shutil.copytree(model_source, model_dest, dirs_exist_ok=True)\n",
    "        print(f\"✅ Model copied to: {model_dest}\")\n",
    "    else:\n",
    "        print(f\"❌ Model not found at: {model_source}\")\n",
    "        return\n",
    "    \n",
    "    # Create requirements file\n",
    "    requirements_content = \"torch>=2.0.0\\ntransformers>=4.30.0\\nnumpy>=1.24.0\\n\"\n",
    "    \n",
    "    with open(f\"{deployment_dir}/requirements.txt\", \"w\") as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    # Create simple inference script\n",
    "    inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple Sentiment Analysis Inference Script\n",
    "Usage: python sentiment_analyzer.py \"Your text here\"\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import sys\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.labels = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        \n",
    "        return {\n",
    "            'sentiment': self.labels[prediction],\n",
    "            'confidence': probabilities[0][prediction].item(),\n",
    "            'probabilities': {\n",
    "                self.labels[i]: prob.item() for i, prob in enumerate(probabilities[0])\n",
    "            }\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python sentiment_analyzer.py 'Your text here'\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    text = \" \".join(sys.argv[1:])\n",
    "    analyzer = SentimentAnalyzer(\"sentiment_model\")\n",
    "    result = analyzer.analyze(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print(\"Probabilities:\")\n",
    "    for sentiment, prob in result['probabilities'].items():\n",
    "        print(f\"  {sentiment}: {prob:.4f}\")\n",
    "'''\n",
    "    \n",
    "    with open(f\"{deployment_dir}/sentiment_analyzer.py\", \"w\") as f:\n",
    "        f.write(inference_script)\n",
    "    \n",
    "    # Create README - SIMPLIFIED WITHOUT CODE BLOCKS\n",
    "    readme_content = \"\"\"# Fine-tuned Sentiment Analysis Model\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Install dependencies:\n",
    "   pip install -r requirements.txt\n",
    "\n",
    "2. Run sentiment analysis:\n",
    "   python sentiment_analyzer.py \"I love this product!\"\n",
    "\n",
    "## Model Details\n",
    "- Base: distilbert-base-uncased\n",
    "- Classes: Negative, Neutral, Positive\n",
    "- Fine-tuned on custom dataset\n",
    "\n",
    "## Usage in Python\n",
    "   from sentiment_analyzer import SentimentAnalyzer\n",
    "   analyzer = SentimentAnalyzer(\"sentiment_model\")\n",
    "   result = analyzer.analyze(\"Your text here\")\n",
    "   print(result)\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{deployment_dir}/README.md\", \"w\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"✅ Deployment package created at: {deployment_dir}\")\n",
    "    print(f\"📁 Contents:\")\n",
    "    print(f\"   - sentiment_model/ (your fine-tuned model)\")\n",
    "    print(f\"   - sentiment_analyzer.py (inference script)\")\n",
    "    print(f\"   - requirements.txt (dependencies)\")\n",
    "    print(f\"   - README.md (usage instructions)\")\n",
    "    print(f\"\\n🚀 Ready for deployment!\")\n",
    "\n",
    "# Export the model\n",
    "export_for_production()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified examples\n",
    "def analyze_errors(predictions, labels, datasets):\n",
    "    test_texts = datasets['test']['text']\n",
    "    \n",
    "    misclassified = []\n",
    "    \n",
    "    for i, (pred, true) in enumerate(zip(predictions, labels)):\n",
    "        if pred != true:\n",
    "            misclassified.append({\n",
    "                'text': test_texts[i],\n",
    "                'predicted': pred,\n",
    "                'true': true,\n",
    "                'predicted_label': ['Negative', 'Neutral', 'Positive'][pred],\n",
    "                'true_label': ['Negative', 'Neutral', 'Positive'][true]\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    error_df = pd.DataFrame(misclassified)\n",
    "    \n",
    "    if len(error_df) > 0:\n",
    "        print(f\"Misclassified examples: {len(error_df)}\")\n",
    "        \n",
    "        # Display some misclassified examples\n",
    "        print(\"\\nSample misclassified texts:\")\n",
    "        for i, row in error_df.head(5).iterrows():\n",
    "            print(f\"\\nText: {row['text']}\")\n",
    "            print(f\"True: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
    "        \n",
    "        # Error patterns\n",
    "        print(\"\\nError Patterns:\")\n",
    "        error_matrix = pd.crosstab(error_df['true_label'], error_df['predicted_label'])\n",
    "        print(error_matrix)\n",
    "        \n",
    "        return error_df\n",
    "    else:\n",
    "        print(\"No misclassified examples!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run error analysis\n",
    "error_df = analyze_errors(predictions, labels, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 FINAL PROJECT STATUS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check key accomplishments\n",
    "accomplishments = [\n",
    "    (\"Data preprocessing completed\", 'datasets' in locals()),\n",
    "    (\"Model trained and saved\", os.path.exists(\"../models/saved_models/fine_tuned_sentiment\")),\n",
    "    (\"Model evaluated\", 'predictions' in locals() and 'labels' in locals()),\n",
    "    (\"Deployment package created\", os.path.exists(\"../models/deployment_package\")),\n",
    "]\n",
    "\n",
    "for accomplishment, status in accomplishments:\n",
    "    print(f\"{'✅' if status else '❌'} {accomplishment}\")\n",
    "\n",
    "if all(status for _, status in accomplishments):\n",
    "    print(\"\\n🎊 CONGRATULATIONS! 🎊\")\n",
    "    print(\"You have successfully completed the sentiment analysis fine-tuning project!\")\n",
    "    print(\"\\nYour fine-tuned model can now analyze sentiment in any text!\")\n",
    "    print(\"\\nNext: Start using your model in your applications! 🚀\")\n",
    "else:\n",
    "    print(\"\\nComplete the missing steps above to finish the project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for production\n",
    "def export_model():\n",
    "    import shutil\n",
    "    \n",
    "    # Copy model to deployment directory\n",
    "    deployment_dir = \"../models/deployment\"\n",
    "    os.makedirs(deployment_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy the fine-tuned model\n",
    "    shutil.copytree(\"../models/saved_models/fine_tuned_sentiment\", \n",
    "                   f\"{deployment_dir}/sentiment_model\", \n",
    "                   dirs_exist_ok=True)\n",
    "    \n",
    "    # Create requirements file for deployment\n",
    "    with open(f\"{deployment_dir}/requirements.txt\", 'w') as f:\n",
    "        f.write(\"torch\\ntransformers\\n\")\n",
    "    \n",
    "    # Create simple inference script\n",
    "    inference_script = '''\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.labels = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    \n",
    "    def predict(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        \n",
    "        return {\n",
    "            'sentiment': self.labels[prediction],\n",
    "            'confidence': probabilities[0][prediction].item(),\n",
    "            'probabilities': {self.labels[i]: prob.item() for i, prob in enumerate(probabilities[0])}\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = SentimentAnalyzer(\"sentiment_model\")\n",
    "    result = analyzer.predict(\"I love this product!\")\n",
    "    print(result)\n",
    "'''\n",
    "    \n",
    "    with open(f\"{deployment_dir}/inference.py\", 'w') as f:\n",
    "        f.write(inference_script)\n",
    "    \n",
    "    print(f\"Model exported to {deployment_dir}\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"- sentiment_model/ (model files)\")\n",
    "    print(\"- requirements.txt\")\n",
    "    print(\"- inference.py (deployment script)\")\n",
    "\n",
    "# Export the model\n",
    "export_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has guided you through:\n",
    "1. **Setup and model download**\n",
    "2. **Data exploration and visualization**\n",
    "3. **Data preprocessing and tokenization**\n",
    "4. **Model training and fine-tuning**\n",
    "5. **Comprehensive evaluation**\n",
    "6. **Real-time inference**\n",
    "7. **Error analysis**\n",
    "8. **Model export for deployment**\n",
    "\n",
    "Your fine-tuned sentiment analysis model is now ready for use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
